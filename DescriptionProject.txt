Multiple GANs have been proposed in recent years, trying to overcome problems existing in the original GAN paper such as Mode Collapse. We chose to work with the f-gan, a family of gan based on the f-divergences [1]. Our project will focus on both a theoritical analysis of the different divergences and how they influence the GAN objective, as well as a practical analysis of the various f-gans on different datasets (MNIST, toy dataset of distributions, CelebA, ...). Depending on the time it takes us to run initial experiments and analysis we would like to do further analysis with the model, for example a study of the prior-manifold mapping, put reinforcement learning in the model, or attempt to use GANs for data augmentation.

[1]Nowozin, S., Cseke, B., & Tomioka, R. (2016). f-gan: Training generative neural samplers using variational divergence minimization. In Advances in neural information processing systems (pp. 271-279).
