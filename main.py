import torch
from torch import optim
from torch.autograd import Variable
from torchvision.utils import save_image
from utils import get_data, visualize_tsne, plot_losses, estimate_mu_var, get_data_q
from f_gan import Generator, Critic, Divergence
import argparse
import numpy as np
import json
import random
import matplotlib.pyplot as plt
import ast
torch.manual_seed(3435)
#MAYBE IT DOESNT LEARN BECAUSE VANISHING GRADIENT
#TODO: PLOT
#TODO: SEE IF VANISHING GRADIENT
#TOTO COMPARE WITH FULL LEARNING

def run_exp(argsdict):
    # Example of usage of the code provided and recommended hyper parameters for training GANs.
    n_iter = argsdict['nb_epoch'] # N training iterations
    lr = 3e-4
    z_dim = argsdict['Gauss_size']
    hidden_dim=(400, 400)

    image_shape=(1, 28, 28)
    encoding='sigmoid'
    # Finding random mu
    mus = []
    sigma=[]
    for gaus in range(argsdict['number_gaussians']):
        # mus.append([random.randint(-1, -1) for _ in range(argsdict['Gauss_size'])])
        mus.append([1.01 for _ in range(argsdict['Gauss_size'])])
        # sigma.append([random.randint(1, 1) for _ in range(argsdict['Gauss_size'])])
        sigma.append([1.8308**2 for _ in range(argsdict['Gauss_size'])])
    mus = torch.tensor(mus)
    sigma=torch.tensor(sigma)

    mus2 = []
    sigma2 = []
    for gaus in range(argsdict['number_gaussians']):
        mus2.append([1.0335 for _ in range(argsdict['Gauss_size'])])
        # mus2.append([random.randint(1, 1) for _ in range(argsdict['Gauss_size'])])
        sigma2.append([1.8236**2 for _ in range(argsdict['Gauss_size'])])
        # sigma2.append([random.randint(4, 4) for _ in range(argsdict['Gauss_size'])])
    mus2 = torch.tensor(mus2)
    sigma2 = torch.tensor(sigma2)
    argsdict['mus'] = mus
    argsdict['sigma']=sigma
    argsdict['musq'] = mus2
    argsdict['sigmaq'] = sigma2

    # Use the GPU if you have one
    if torch.cuda.is_available():
        print("Using the GPU")
        device = torch.device("cuda")
    else:
        print("WARNING: You are about to run on cpu, and this will likely run out \
          of memory. \n You can try setting batch_size=1 to reduce memory usage")
        device = torch.device("cpu")

    bb = torch.zeros((argsdict['batch_size'], argsdict['Gauss_size']))
    # Choose random gaussian
    for j in range(argsdict['batch_size']):
        gaus = random.randint(0, argsdict['number_gaussians'] - 1)
        mu = argsdict['musq'][gaus]
        sigma = argsdict['sigmaq'][gaus]
        point = sigma * torch.randn(argsdict['Gauss_size']) + mu
        # print(point)

        bb[j] = point
    # print(bb[0])
    estimatedMu=torch.mean(bb, dim=0)
    estimatedVar=torch.std(bb, dim=0, unbiased=True)

    losses=Divergence(argsdict['divergence'])

    print(f"real mu: {argsdict['mus']}, real sigma: {argsdict['sigma']}")
    print(f"estimated mu: {estimatedMu}, generated sigma: {estimatedVar}")
    fdiv=losses.AnalyticDiv(estimatedMu.unsqueeze(dim=0), argsdict['mus'], estimatedVar.unsqueeze(dim=0), argsdict['sigma'])
    Realfdiv=losses.AnalyticDiv(argsdict['musq'], argsdict['mus'], argsdict['sigmaq'], argsdict['sigma'])
    print(f"f divergence {losses.AnalyticDiv(estimatedMu.unsqueeze(dim=0), argsdict['mus'], estimatedVar.unsqueeze(dim=0), argsdict['sigma'])}")
    print(f"Real f divergence {losses.AnalyticDiv(argsdict['musq'], argsdict['mus'], argsdict['sigmaq'], argsdict['sigma'])}")
    # Update the losses plot every 5 epochs
    # if epoch%5==0 and epoch!=0:
    #     plot_losses(argsdict, epoch+1, show_plot=0)
                  
    # plot_losses(argsdict, n_iter)

    return fdiv, Realfdiv

#BS: 500: 0.4703 vs 0.44315
#BS: 5000
#I need to train it before so that's its actually a gaussian, and then try with different batch size

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Project for IFT6269 on fgans')
    parser.add_argument('--dataset', type=str, default='Gaussian',
                        help='Dataset you want to use. Options include MNIST, svhn, Gaussian, and CIFAR')
    parser.add_argument('--divergence', type=str, default='forward_kl',
                        help='divergence to use. Options include total_variation, forward_kl, reverse_kl, pearson, hellinger, jensen_shannon, alpha_div or all')
    parser.add_argument('--Gauss_size', type=int, default=1, help='The size of the Gaussian we generate')
    parser.add_argument('--number_gaussians', type=int, default='1', help='The number of Gaussian we generate')
    parser.add_argument('--dataset_size', type=int, default=50000, help='the total number of points generated by the gaussian')
    parser.add_argument('--num_gen', type=int, default=5, help='number of point generated by both the dataset and generator')
    parser.add_argument('--threshold', type=float, default=0.5,help='threshold after which the data point is considered as part of the generated distribution')
    #Training options
    parser.add_argument('--nb_epoch', type=int, default=100)
    parser.add_argument('--batch_size', type=int, default=500, help='batch size for training and testing')
    parser.add_argument('--modified_loss', action='store_true', help='use the loss of section 3.2 instead of the original formulation')
    parser.add_argument('--hidden_crit_size', type=int, default=32)
    parser.add_argument('--visualize', action='store_true', help='save visualization of the datasets using t-sne')
    parser.add_argument('--use_cuda', action='store_true', help='Use gpu')
    parser.add_argument('--train_generator', action='store_true', help='train the generator to match the distribution')
    args = parser.parse_args()

    argsdict = args.__dict__
    if argsdict['divergence']=='all':
        divergence=['total_variation', 'forward_kl', 'reverse_kl', 'pearson', 'hellinger', 'jensen_shannon','alpha_div']
        for dd in divergence:
            print(dd)
            argsdict['divergence']=dd
            run_exp(argsdict)
    elif argsdict['train_generator']:
        run_exp(argsdict)
    else:
        estimated=[]
        true=[]
        arr=[i for i in range(5, 50000, 100)]
        arr=[100]
        for bs in arr:
            argsdict['batch_size']=bs
            argsdict['nb_epoch']=25
            argsdict['dataset_size']=bs*20
            print(bs)
            fake, real=run_exp(argsdict)
            estimated.append(fake.item())
            true.append(real.item())
        # with open(f"{argsdict['dataset']}_IMGS/{argsdict['divergence']}/ArtificialMC.txt", "w") as f:
        #         json.dump({"Estimated":estimated, "True":true, "batch_size":arr}, f)